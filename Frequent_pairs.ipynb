{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ed2696",
   "metadata": {},
   "source": [
    "# Finding Similar Items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1f1a7",
   "metadata": {},
   "source": [
    "## 1.\tIntroduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba526dd",
   "metadata": {},
   "source": [
    "### Frequent pairs of items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f60919",
   "metadata": {},
   "source": [
    "- Extract a list of the authors or editors per publication from the ACL Anthology dataset (https://aclanthology.org/) and create baskets and perform a search on the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c15a8fb",
   "metadata": {},
   "source": [
    "1. Find the frequent pair of items (2-tuples) using the naïve, A-priori and PCY algorithms. For each of these compare the time of execution and results for supports s=10, 50, 100. Comment your results. \n",
    "\n",
    "2. For the PCY algorithm, create up to 5 compact hash tables. What is  the difference in results and time of execution for 1,2,3,4 and 5 tables? Comment your results.\n",
    "\n",
    "3. Find the final list of k-frequent items (k-tuples) for k=3 and 4. Experiment a bit and describe the best value for the support in each case. *Warning*: You can use any of the three algorithms, but be careful, because the algorithm can take too long if you don't chose it properly (well, basically don't use the naïve approach ;)).\n",
    "\n",
    "4. Using one of the results of the previous items, for one k (k=2 or 3) find the possible clusters using the 1-NN criteria. Comment your results.\n",
    "\n",
    "> 1-NN means that if you have a tuple {A,B,C} and {C,E,F} then because they share one element {C}, then they belong to the same cluster  {A,B,C,E,F}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5a302",
   "metadata": {},
   "source": [
    "-\tDefine all and only used package imports below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb0f6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alext\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests # to download the dataset\n",
    "import gzip\n",
    "import shutil # to extract the gz file\n",
    "import re # for text cleaning\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "from nltk.corpus import stopwords # calculation of stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "# ! pip install pylatexenc\n",
    "from pylatexenc.latex2text import LatexNodes2Text # fix umlaut vocals in names\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33381fdb",
   "metadata": {},
   "source": [
    "## 2.\tELT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb9642",
   "metadata": {},
   "source": [
    "### Extract, Load and Transform of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee851872",
   "metadata": {},
   "source": [
    "- In your code data should be retrieved from an online source, NOT from your local drive, otherwise, nobody can run your code without additional effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd91c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data \n",
    "url = 'https://aclanthology.org/anthology+abstracts.bib.gz'\n",
    "filename = url.split(\"/\")[-1]\n",
    "with open(filename, \"wb\") as f:\n",
    "    r = requests.get(url)\n",
    "    f.write(r.content)\n",
    "\n",
    "# Extract the gz file\n",
    "with gzip.open('anthology+abstracts.bib.gz', 'rb') as f_in:\n",
    "    with open('anthology+abstracts.bib', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "93d963b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73129 baskets of authors were found in the file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find all the rows in the file that contain an abstract and laod the text to a list\n",
    "authors = []\n",
    "with open(\"anthology+abstracts.bib\", \"r\",encoding=\"UTF-8\") as f:\n",
    "    line = f.readline()\n",
    "    while(line != ''):\n",
    "          \n",
    "        if line.__contains__('author =') or line.__contains__('editor ='):\n",
    "            while not line.endswith('\",\\n'):\n",
    "                line = line+f.readline()\n",
    "            # something to clean\n",
    "            line = LatexNodes2Text().latex_to_text(line) # fix umlaut vocals    \n",
    "            authors.append(line)\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "\n",
    "print(\"{} baskets of authors were found in the file.\".format(len(authors)))\n",
    "#the issue here is that only the first row after author/editor is stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "976a4d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    author = \"Singh, Sumer  and\\n      Li, Sheng\",\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f8b1ed02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning, 18181 rows of authors were remaining.\n"
     ]
    }
   ],
   "source": [
    "# Some cleaning\n",
    "\n",
    "minletters = 5 \n",
    "authors_clean = []\n",
    "\n",
    "for a in authors: \n",
    "    if len(a) > minletters and len(re.findall('[a-zA-Z]',a)) >0.6*len(a):\n",
    "        authors_clean.append(a) \n",
    "print(\"After cleaning, {} rows of authors were remaining.\".format(len(authors_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2715c409",
   "metadata": {},
   "source": [
    "### Report the essential description of data.\n",
    "-\tDon’t print out dozens of raw lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "de8002ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>18181.0</td>\n",
       "      <td>28.208019</td>\n",
       "      <td>20.392735</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>26.00</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_count</th>\n",
       "      <td>18181.0</td>\n",
       "      <td>106.355811</td>\n",
       "      <td>74.479120</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>91.00</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>1767.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_word</th>\n",
       "      <td>18181.0</td>\n",
       "      <td>6.334465</td>\n",
       "      <td>0.526591</td>\n",
       "      <td>4.454545</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>6.571429</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopwords</th>\n",
       "      <td>18181.0</td>\n",
       "      <td>2.459161</td>\n",
       "      <td>2.674914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper</th>\n",
       "      <td>18181.0</td>\n",
       "      <td>0.134206</td>\n",
       "      <td>0.419580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count        mean        std        min   25%    50%  \\\n",
       "word_count  18181.0   28.208019  20.392735   8.000000  17.0  26.00   \n",
       "char_count  18181.0  106.355811  74.479120  32.000000  62.0  91.00   \n",
       "avg_word    18181.0    6.334465   0.526591   4.454545   6.0   6.25   \n",
       "stopwords   18181.0    2.459161   2.674914   0.000000   1.0   2.00   \n",
       "upper       18181.0    0.134206   0.419580   0.000000   0.0   0.00   \n",
       "\n",
       "                   75%     max  \n",
       "word_count   35.000000   560.0  \n",
       "char_count  132.000000  1767.0  \n",
       "avg_word      6.571429    10.0  \n",
       "stopwords     3.000000    60.0  \n",
       "upper         0.000000     7.0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(authors_clean, columns=['authors'])\n",
    "# Number of words\n",
    "data['word_count'] = data['authors'].apply(lambda x: len(str(x).split(\" \")))\n",
    "data[['authors','word_count']]\n",
    "\n",
    "#Number of characters\n",
    "data['char_count'] = data['authors'].str.len() ## this also includes spaces\n",
    "data[['authors','char_count']]\n",
    "\n",
    "# Average word length\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "data['avg_word'] = data['authors'].apply(lambda x: avg_word(x))\n",
    "\n",
    "# Number of stop words \n",
    "stop = stopwords.words('english')\n",
    "data['stopwords'] = data['authors'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "# Number of Uppercase words\n",
    "data['upper'] = data['authors'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "\n",
    "# Descriptive statistics of the DataFrame\n",
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684bc23c",
   "metadata": {},
   "source": [
    "## 3.\tModeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911e083",
   "metadata": {},
   "source": [
    "### Prepare analytics here and construct all the data objects you will use in your report.\n",
    "•\tWrite functions and classes to simplify tasks. Do not repeat yourself.\n",
    "\n",
    "•\tAvoid output.\n",
    "\n",
    "•\tRefactor your code until it’s clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d443eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdata(k, fname=\"data/data.txt\", report=False):\n",
    "    C_k = []\n",
    "    b = 0\n",
    "\n",
    "    for line in fname:\n",
    "        line = line.replace('\\n', '')  # remove newline symbol\n",
    "        if report:\n",
    "            print(line)\n",
    "         \n",
    "        if line != \"\":\n",
    "            # gather all items in one basket\n",
    "            C_k.append(line)\n",
    "        else:\n",
    "            # end of basket, report all itemsets\n",
    "            for itemset in itertools.combinations(C_k, k):\n",
    "                yield frozenset(itemset)\n",
    "            C_k = []\n",
    "                \n",
    "            if report:\n",
    "                print(\"\")\n",
    "\n",
    "            # report progress\n",
    "            # print every 1000th element to reduce clutter\n",
    "            if report:\n",
    "                if b % 1000 == 0:\n",
    "                    print('processing bin ', b)\n",
    "                b += 1\n",
    "\n",
    "    # last basket\n",
    "    if len(C_k) > 0:\n",
    "        for itemset in itertools.combinations(C_k, k):\n",
    "            yield frozenset(itemset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f6f6d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33590 items\n",
      "3328 items with >5 occurances\n"
     ]
    }
   ],
   "source": [
    "N = 5  # frequency threshold\n",
    "\n",
    "\n",
    "# find frequent 1-tuples (individual items)\n",
    "C1 = {}\n",
    "for key in readdata(k=1, fname=data[\"authors\"], report=False):\n",
    "    if key not in C1:\n",
    "        C1[key] = 1\n",
    "    else:\n",
    "        C1[key] += 1    \n",
    "        \n",
    "print(\"{} items\".format(len(C1)))\n",
    "\n",
    "# filter stage\n",
    "L1 = {}\n",
    "for key, count in C1.items():\n",
    "    if count >= N:\n",
    "        L1[key] = count\n",
    "print('{} items with >{} occurances'.format(len(L1), N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ccd374d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.0s for k=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{frozenset({'a', 'd'}): 8,\n",
       " frozenset({'d', 't'}): 8,\n",
       " frozenset({'/', 'd'}): 2,\n",
       " frozenset({'d'}): 1,\n",
       " frozenset({'.', 'd'}): 2,\n",
       " frozenset({'d', 'x'}): 2,\n",
       " frozenset({'a', 't'}): 16,\n",
       " frozenset({'a'}): 6,\n",
       " frozenset({'/', 'a'}): 4,\n",
       " frozenset({'.', 'a'}): 4,\n",
       " frozenset({'a', 'x'}): 4,\n",
       " frozenset({'/', 't'}): 4,\n",
       " frozenset({'t'}): 6,\n",
       " frozenset({'.', 't'}): 4,\n",
       " frozenset({'t', 'x'}): 4,\n",
       " frozenset({'.', '/'}): 1,\n",
       " frozenset({'/', 'x'}): 1,\n",
       " frozenset({'.', 'x'}): 1}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def get_C(k):\n",
    "\n",
    "    start = time.time()\n",
    "    C = {}\n",
    "    for key in readdata(k):  # False report\n",
    "        if key not in C:\n",
    "            C[key] = 1\n",
    "        else:\n",
    "            C[key] += 1\n",
    "    print(\"Took {}s for k={}\".format((time.time() - start), k))\n",
    "    return C\n",
    "\n",
    "get_C(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e45e114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.', 'We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.')\n",
      "('Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.', \"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to {`}memes in the wild{'}. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that {`}memes in the wild{'} differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than {`}traditional memes{'}, including screenshots of conversations or text on a plain background. This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.\")\n",
      "('Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.', 'Content moderation is often performed by a collaboration between humans and machine learning models. However, it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance. This work presents a rigorous study of this problem, focusing on an approach that incorporates model uncertainty into the collaborative process. First, we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator, quantifying how efficiently the combined system utilizes human decisions. Using these metrics, we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies. We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance. Our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation, as well as the utility of uncertainty estimation in this domain.')\n",
      "('Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.', 'As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually an- notated for abusive language. The resource ad- dress a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification.')\n",
      "('Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.', 'Social media texts such as blog posts, comments, and tweets often contain offensive languages including racial hate speech comments, personal attacks, and sexual harassment. Detecting inappropriate use of language is, therefore, of utmost importance for the safety of the users as well as for suppressing hateful conduct and aggression. Existing approaches to this problem are mostly available for resource-rich languages such as English and German. In this paper, we characterize the offensive language in Nepali, a low-resource language, highlighting the challenges that need to be addressed for processing Nepali social media text. We also present experiments for detecting offensive language using supervised machine learning. Besides contributing the first baseline approaches of detecting offensive language in Nepali, we also release human annotated data sets to encourage future research on this crucial topic.')\n",
      "('Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.', 'Hate speech-related lexicons have been proved to be useful for many tasks such as data collection and classification. However, existing Portuguese lexicons do not distinguish between European and Brazilian Portuguese, and do not include neutral terms that are potentially useful to detect a broader spectrum of content referring to minorities. In this work, we present MIN{\\\\_}PT, a new European Portuguese Lexicon for Minorities-Related Terms specifically designed to tackle the limitations of existing resources. We describe the data collection and annotation process, discuss the limitation and ethical concerns, and prove the utility of the resource by applying it to a use case for the Portuguese 2021 presidential elections.')\n",
      "('We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.', \"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to {`}memes in the wild{'}. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that {`}memes in the wild{'} differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than {`}traditional memes{'}, including screenshots of conversations or text on a plain background. This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.\")\n",
      "('We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.', 'Content moderation is often performed by a collaboration between humans and machine learning models. However, it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance. This work presents a rigorous study of this problem, focusing on an approach that incorporates model uncertainty into the collaborative process. First, we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator, quantifying how efficiently the combined system utilizes human decisions. Using these metrics, we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies. We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance. Our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation, as well as the utility of uncertainty estimation in this domain.')\n",
      "('We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.', 'As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually an- notated for abusive language. The resource ad- dress a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification.')\n",
      "('We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.', 'Social media texts such as blog posts, comments, and tweets often contain offensive languages including racial hate speech comments, personal attacks, and sexual harassment. Detecting inappropriate use of language is, therefore, of utmost importance for the safety of the users as well as for suppressing hateful conduct and aggression. Existing approaches to this problem are mostly available for resource-rich languages such as English and German. In this paper, we characterize the offensive language in Nepali, a low-resource language, highlighting the challenges that need to be addressed for processing Nepali social media text. We also present experiments for detecting offensive language using supervised machine learning. Besides contributing the first baseline approaches of detecting offensive language in Nepali, we also release human annotated data sets to encourage future research on this crucial topic.')\n",
      "('We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.', 'Hate speech-related lexicons have been proved to be useful for many tasks such as data collection and classification. However, existing Portuguese lexicons do not distinguish between European and Brazilian Portuguese, and do not include neutral terms that are potentially useful to detect a broader spectrum of content referring to minorities. In this work, we present MIN{\\\\_}PT, a new European Portuguese Lexicon for Minorities-Related Terms specifically designed to tackle the limitations of existing resources. We describe the data collection and annotation process, discuss the limitation and ethical concerns, and prove the utility of the resource by applying it to a use case for the Portuguese 2021 presidential elections.')\n",
      "(\"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to {`}memes in the wild{'}. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that {`}memes in the wild{'} differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than {`}traditional memes{'}, including screenshots of conversations or text on a plain background. This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.\", 'Content moderation is often performed by a collaboration between humans and machine learning models. However, it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance. This work presents a rigorous study of this problem, focusing on an approach that incorporates model uncertainty into the collaborative process. First, we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator, quantifying how efficiently the combined system utilizes human decisions. Using these metrics, we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies. We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance. Our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation, as well as the utility of uncertainty estimation in this domain.')\n",
      "(\"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to {`}memes in the wild{'}. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that {`}memes in the wild{'} differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than {`}traditional memes{'}, including screenshots of conversations or text on a plain background. This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.\", 'As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually an- notated for abusive language. The resource ad- dress a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification.')\n",
      "(\"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to {`}memes in the wild{'}. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that {`}memes in the wild{'} differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than {`}traditional memes{'}, including screenshots of conversations or text on a plain background. This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.\", 'Social media texts such as blog posts, comments, and tweets often contain offensive languages including racial hate speech comments, personal attacks, and sexual harassment. Detecting inappropriate use of language is, therefore, of utmost importance for the safety of the users as well as for suppressing hateful conduct and aggression. Existing approaches to this problem are mostly available for resource-rich languages such as English and German. In this paper, we characterize the offensive language in Nepali, a low-resource language, highlighting the challenges that need to be addressed for processing Nepali social media text. We also present experiments for detecting offensive language using supervised machine learning. Besides contributing the first baseline approaches of detecting offensive language in Nepali, we also release human annotated data sets to encourage future research on this crucial topic.')\n",
      "(\"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to {`}memes in the wild{'}. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that {`}memes in the wild{'} differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than {`}traditional memes{'}, including screenshots of conversations or text on a plain background. This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.\", 'Hate speech-related lexicons have been proved to be useful for many tasks such as data collection and classification. However, existing Portuguese lexicons do not distinguish between European and Brazilian Portuguese, and do not include neutral terms that are potentially useful to detect a broader spectrum of content referring to minorities. In this work, we present MIN{\\\\_}PT, a new European Portuguese Lexicon for Minorities-Related Terms specifically designed to tackle the limitations of existing resources. We describe the data collection and annotation process, discuss the limitation and ethical concerns, and prove the utility of the resource by applying it to a use case for the Portuguese 2021 presidential elections.')\n",
      "('Content moderation is often performed by a collaboration between humans and machine learning models. However, it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance. This work presents a rigorous study of this problem, focusing on an approach that incorporates model uncertainty into the collaborative process. First, we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator, quantifying how efficiently the combined system utilizes human decisions. Using these metrics, we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies. We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance. Our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation, as well as the utility of uncertainty estimation in this domain.', 'As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually an- notated for abusive language. The resource ad- dress a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification.')\n",
      "('Content moderation is often performed by a collaboration between humans and machine learning models. However, it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance. This work presents a rigorous study of this problem, focusing on an approach that incorporates model uncertainty into the collaborative process. First, we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator, quantifying how efficiently the combined system utilizes human decisions. Using these metrics, we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies. We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance. Our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation, as well as the utility of uncertainty estimation in this domain.', 'Social media texts such as blog posts, comments, and tweets often contain offensive languages including racial hate speech comments, personal attacks, and sexual harassment. Detecting inappropriate use of language is, therefore, of utmost importance for the safety of the users as well as for suppressing hateful conduct and aggression. Existing approaches to this problem are mostly available for resource-rich languages such as English and German. In this paper, we characterize the offensive language in Nepali, a low-resource language, highlighting the challenges that need to be addressed for processing Nepali social media text. We also present experiments for detecting offensive language using supervised machine learning. Besides contributing the first baseline approaches of detecting offensive language in Nepali, we also release human annotated data sets to encourage future research on this crucial topic.')\n",
      "('Content moderation is often performed by a collaboration between humans and machine learning models. However, it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance. This work presents a rigorous study of this problem, focusing on an approach that incorporates model uncertainty into the collaborative process. First, we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator, quantifying how efficiently the combined system utilizes human decisions. Using these metrics, we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies. We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance. Our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation, as well as the utility of uncertainty estimation in this domain.', 'Hate speech-related lexicons have been proved to be useful for many tasks such as data collection and classification. However, existing Portuguese lexicons do not distinguish between European and Brazilian Portuguese, and do not include neutral terms that are potentially useful to detect a broader spectrum of content referring to minorities. In this work, we present MIN{\\\\_}PT, a new European Portuguese Lexicon for Minorities-Related Terms specifically designed to tackle the limitations of existing resources. We describe the data collection and annotation process, discuss the limitation and ethical concerns, and prove the utility of the resource by applying it to a use case for the Portuguese 2021 presidential elections.')\n",
      "('As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually an- notated for abusive language. The resource ad- dress a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification.', 'Social media texts such as blog posts, comments, and tweets often contain offensive languages including racial hate speech comments, personal attacks, and sexual harassment. Detecting inappropriate use of language is, therefore, of utmost importance for the safety of the users as well as for suppressing hateful conduct and aggression. Existing approaches to this problem are mostly available for resource-rich languages such as English and German. In this paper, we characterize the offensive language in Nepali, a low-resource language, highlighting the challenges that need to be addressed for processing Nepali social media text. We also present experiments for detecting offensive language using supervised machine learning. Besides contributing the first baseline approaches of detecting offensive language in Nepali, we also release human annotated data sets to encourage future research on this crucial topic.')\n",
      "('As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually an- notated for abusive language. The resource ad- dress a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification.', 'Hate speech-related lexicons have been proved to be useful for many tasks such as data collection and classification. However, existing Portuguese lexicons do not distinguish between European and Brazilian Portuguese, and do not include neutral terms that are potentially useful to detect a broader spectrum of content referring to minorities. In this work, we present MIN{\\\\_}PT, a new European Portuguese Lexicon for Minorities-Related Terms specifically designed to tackle the limitations of existing resources. We describe the data collection and annotation process, discuss the limitation and ethical concerns, and prove the utility of the resource by applying it to a use case for the Portuguese 2021 presidential elections.')\n",
      "('Social media texts such as blog posts, comments, and tweets often contain offensive languages including racial hate speech comments, personal attacks, and sexual harassment. Detecting inappropriate use of language is, therefore, of utmost importance for the safety of the users as well as for suppressing hateful conduct and aggression. Existing approaches to this problem are mostly available for resource-rich languages such as English and German. In this paper, we characterize the offensive language in Nepali, a low-resource language, highlighting the challenges that need to be addressed for processing Nepali social media text. We also present experiments for detecting offensive language using supervised machine learning. Besides contributing the first baseline approaches of detecting offensive language in Nepali, we also release human annotated data sets to encourage future research on this crucial topic.', 'Hate speech-related lexicons have been proved to be useful for many tasks such as data collection and classification. However, existing Portuguese lexicons do not distinguish between European and Brazilian Portuguese, and do not include neutral terms that are potentially useful to detect a broader spectrum of content referring to minorities. In this work, we present MIN{\\\\_}PT, a new European Portuguese Lexicon for Minorities-Related Terms specifically designed to tackle the limitations of existing resources. We describe the data collection and annotation process, discuss the limitation and ethical concerns, and prove the utility of the resource by applying it to a use case for the Portuguese 2021 presidential elections.')\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for c in itertools.combinations(data[\"abstracts\"][1:8], 2):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4582ad63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.combinations at 0x1cab0dc1450>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itertools.combinations(data[\"abstracts\"][1:8],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db38557",
   "metadata": {},
   "source": [
    "## 4.\tResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e83a47",
   "metadata": {},
   "source": [
    "•\tPrint out relevant tables nicely, display well-annotated charts and explain if needed in plain English.\n",
    "•\tUse minimum code here, just output-functions’ calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd570aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae10d5d3",
   "metadata": {},
   "source": [
    "## 5.\tConclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6c727f",
   "metadata": {},
   "source": [
    "•\tSummarize your findings here in 5...10 lines of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0c8ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in Frequent_pairs.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "246051e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in Frequent_pairs.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main f5ce580] some progress\n",
      " 1 file changed, 1118 insertions(+), 1068 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/AlexTouvras/FindingSimilarItems\n",
      "   27d8ee1..f5ce580  main -> main\n"
     ]
    }
   ],
   "source": [
    "# ! git add Frequent_pairs.ipynb\n",
    "# ! git commit -m \"some progress\"\n",
    "# ! git push "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
