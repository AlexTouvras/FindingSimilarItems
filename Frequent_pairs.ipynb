{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2192442d",
   "metadata": {},
   "source": [
    "# Finding Similar Items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c02f489",
   "metadata": {},
   "source": [
    "## 1.\tIntroduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd6f09",
   "metadata": {},
   "source": [
    "### Frequent pairs of items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab5778",
   "metadata": {},
   "source": [
    "- Extract a list of the authors or editors per publication from the ACL Anthology dataset (https://aclanthology.org/) and create baskets and perform a search on the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce57133",
   "metadata": {},
   "source": [
    "1. Find the frequent pair of items (2-tuples) using the naïve, A-priori and PCY algorithms. For each of these compare the time of execution and results for supports s=10, 50, 100. Comment your results.\n",
    "\n",
    "2. For the PCY algorithm, create up to 5 compact hash tables. What is  the difference in results and time of execution for 1,2,3,4 and 5 tables? Comment your results.\n",
    "\n",
    "3. Find the final list of k-frequent items (k-tuples) for k=3 and 4. Experiment a bit and describe the best value for the support in each case. *Warning*: You can use any of the three algorithms, but be careful, because the algorithm can take too long if you don't chose it properly (well, basically don't use the naïve approach ;)).\n",
    "\n",
    "4. Using one of the results of the previous items, for one k (k=2 or 3) find the possible clusters using the 1-NN criteria. Comment your results.\n",
    "\n",
    "> 1-NN means that if you have a tuple {A,B,C} and {C,E,F} then because they share one element {C}, then they belong to the same cluster  {A,B,C,E,F}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a6a88",
   "metadata": {},
   "source": [
    "-\tDefine all and only used package imports below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d34dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # to download the dataset\n",
    "import gzip\n",
    "import shutil # to extract the gz file\n",
    "import re # for text cleaning\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f3f0b0",
   "metadata": {},
   "source": [
    "## 2.\tELT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc1293",
   "metadata": {},
   "source": [
    "### Extract, Load and Transform of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ada65",
   "metadata": {},
   "source": [
    "- In your code data should be retrieved from an online source, NOT from your local drive, otherwise, nobody can run your code without additional effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c4a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data \n",
    "url = 'https://aclanthology.org/anthology+abstracts.bib.gz'\n",
    "filename = url.split(\"/\")[-1]\n",
    "with open(filename, \"wb\") as f:\n",
    "    r = requests.get(url)\n",
    "    f.write(r.content)\n",
    "\n",
    "# Extract the gz file\n",
    "with gzip.open('anthology+abstracts.bib.gz', 'rb') as f_in:\n",
    "    with open('anthology+abstracts.bib', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ba2e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73199 baskets of authors were found in the file.\n"
     ]
    }
   ],
   "source": [
    "# Find all the rows in the file that contain an author/editor and load the text to a list\n",
    "end = set(['\",\\n', '}\\n'])\n",
    "authors = []\n",
    "with open(\"anthology+abstracts.bib\", \"r\",encoding=\"UTF-8\") as f:\n",
    "    line = f.readline()\n",
    "    while(line != ''):\n",
    "\n",
    "        if line.__contains__('author =') or line.__contains__('editor ='):\n",
    "            while not (line.endswith('\",\\n')|line.endswith('},\\n')):\n",
    "                line = line+f.readline()\n",
    "            # something to clean\n",
    "#           line = LatexNodes2Text().latex_to_text(line) # fix umlaut vocals; this part takes some time to run    \n",
    "            line = re.sub('    editor = \"|    author = \"|\",|\\n|    author = {|    editor = {','',line)\n",
    "            line = re.sub(',','',line)\n",
    "            line = re.sub('  and      ',', ',line)\n",
    "            authors.append(line)\n",
    "            \n",
    "        line = f.readline()\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "print(\"{} baskets of authors were found in the file.\".format(len(authors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7b79a",
   "metadata": {},
   "source": [
    "#In the above task, we have done the data mining & cleaning activity. And we were able to consolidate only authors & editors into one text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6ea5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Singh Sumer, Li Sheng'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b11b30",
   "metadata": {},
   "source": [
    "#Wtrite function to create frozen sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a88d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdata(k, fname=authors, report=False):\n",
    "    C_k = []\n",
    "    b = 0\n",
    "\n",
    "    for line in fname:\n",
    "        if report:\n",
    "            print(line)\n",
    "         \n",
    "        # gather all items in one basket\n",
    "        C_k.append(line.split(\", \"))\n",
    "\n",
    "        # end of basket, report all itemsets\n",
    "        for author in C_k:\n",
    "            for itemset in itertools.combinations(author, k):\n",
    "                yield frozenset(itemset)\n",
    "            C_k = []\n",
    "                \n",
    "        if report:\n",
    "            print(\"\")\n",
    "\n",
    "    # last basket\n",
    "    if len(C_k) > 0:\n",
    "        for itemset in itertools.combinations(C_k, k):\n",
    "            yield frozenset(itemset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a2b9e",
   "metadata": {},
   "source": [
    "### Report the essential description of data.\n",
    "-\tDon’t print out dozens of raw lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6906d8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mostafazadeh Davani Aida, Kiela Douwe, Lambert Mathias, Vidgen Bertie, Prabhakaran Vinodkumar, Waseem Zeerak\n",
      "frozenset({'Kiela Douwe', 'Mostafazadeh Davani Aida'})\n",
      "frozenset({'Lambert Mathias', 'Mostafazadeh Davani Aida'})\n",
      "frozenset({'Vidgen Bertie', 'Mostafazadeh Davani Aida'})\n",
      "frozenset({'Prabhakaran Vinodkumar', 'Mostafazadeh Davani Aida'})\n",
      "frozenset({'Waseem Zeerak', 'Mostafazadeh Davani Aida'})\n"
     ]
    }
   ],
   "source": [
    "nitems = 5\n",
    "for C_k in readdata(k=2, report=True):\n",
    "    print(C_k)\n",
    "    nitems -= 1\n",
    "    \n",
    "    if nitems == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd81818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair of elements\n",
    "import time\n",
    "\n",
    "\n",
    "def get_C(k):\n",
    "\n",
    "    start = time.time()\n",
    "    C = {}\n",
    "    for key in readdata(k):  # False report\n",
    "        if key not in C:\n",
    "            C[key] = 1\n",
    "        else:\n",
    "            C[key] += 1\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "C1 = get_C(1)\n",
    "C2 = get_C(2)\n",
    "C3 = get_C(3)\n",
    "C4 = get_C(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67791081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64850 271422 784913 4376253\n"
     ]
    }
   ],
   "source": [
    "print(len(C1),len(C2),len(C3),len(C4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ed4e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'Kiela Douwe', 'Mostafazadeh Davani Aida'}) 2\n",
      "frozenset({'Lambert Mathias', 'Mostafazadeh Davani Aida'}) 1\n",
      "frozenset({'Vidgen Bertie', 'Mostafazadeh Davani Aida'}) 2\n",
      "frozenset({'Prabhakaran Vinodkumar', 'Mostafazadeh Davani Aida'}) 3\n",
      "frozenset({'Waseem Zeerak', 'Mostafazadeh Davani Aida'}) 2\n"
     ]
    }
   ],
   "source": [
    "for (ck, n), _ in zip(C2.items(), range(5)):\n",
    "    print(ck,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79931742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'Lambert Mathias', 'Kiela Douwe', 'Mostafazadeh Davani Aida'}) 1\n",
      "frozenset({'Vidgen Bertie', 'Kiela Douwe', 'Mostafazadeh Davani Aida'}) 2\n",
      "frozenset({'Prabhakaran Vinodkumar', 'Kiela Douwe', 'Mostafazadeh Davani Aida'}) 2\n",
      "frozenset({'Waseem Zeerak', 'Kiela Douwe', 'Mostafazadeh Davani Aida'}) 2\n",
      "frozenset({'Vidgen Bertie', 'Lambert Mathias', 'Mostafazadeh Davani Aida'}) 1\n"
     ]
    }
   ],
   "source": [
    "for (ck, n), _ in zip(C3.items(), range(5)):\n",
    "    print(ck,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d51e1b",
   "metadata": {},
   "source": [
    "## 3.\tModeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08962e72",
   "metadata": {},
   "source": [
    "### Prepare analytics here and construct all the data objects you will use in your report.\n",
    "•\tWrite functions and classes to simplify tasks. Do not repeat yourself.\n",
    "\n",
    "•\tAvoid output.\n",
    "\n",
    "•\tRefactor your code until it’s clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c51744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interesting_pairs(s):\n",
    "    L2 = {}\n",
    "    C2 = get_C(2)\n",
    "    for key, n in C2.items():\n",
    "        if n >= s:\n",
    "            L2[key] = n\n",
    "    L2 = [elem for elem in list(L2) if len(elem) > 1] \n",
    "\n",
    "    for i in range(len(L2)):\n",
    "\n",
    "        A, B = list(L2[i])\n",
    "        support_AB = C2[frozenset([A, B])]\n",
    "        support_A = C1[frozenset([A])]\n",
    "        conf_A_leads_to_B = support_AB / support_A\n",
    "\n",
    "        support_B = C1[frozenset([B])]\n",
    "        prob_B = support_B / nbaskets\n",
    "\n",
    "        interest_A_leads_to_B = conf_A_leads_to_B - prob_B\n",
    "\n",
    "        if interest_A_leads_to_B > 0.7:\n",
    "            print(\"{} --> {} with interest {:3f}\".format(A, B,\n",
    "                                                         interest_A_leads_to_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2302f7",
   "metadata": {},
   "source": [
    "•\tNaive, Apriori and PCY for 2-tuple frequent item sets (k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff1ab5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_method(s):\n",
    "    C2 = get_C(2)\n",
    "    t = time.time()\n",
    "    L2_naive = {}\n",
    "    for key, n in C2.items():\n",
    "        if n >= s:\n",
    "            L2_naive[key] = n\n",
    "    t2 = round(time.time() - t,3)\n",
    "    print('Naive:    {} items with >{} occurances. Calculated in {} sec'.format(len(L2_naive), s, t2))\n",
    "\n",
    "def Apriori(s):\n",
    "    t = time.time()\n",
    "    # find frequent 1-tuples (individual items)\n",
    "    C1 = {}\n",
    "    for key in readdata(k=1, report=False):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "        \n",
    "    # filter stage\n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= s:\n",
    "            L1[key] = count\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()]) # List comprehensions in python\n",
    "    \n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata(k=2):\n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "\n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= s:\n",
    "            L2[key] = count\n",
    "    t2 = round(time.time() - t,3)\n",
    "    print('A-priori: {} items with >{} occurances. Calculated in {} sec'.format(len(L2), s, t2))\n",
    "    \n",
    "def PCY(s):\n",
    "    t = time.time()\n",
    "    # find frequent 1-tuples (individual items)\n",
    "    C1 = {}\n",
    "    for key in readdata(k=1, report=False):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "            \n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= s:\n",
    "            L1[key] = count\n",
    "\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()]) # List comprehensions in python\n",
    "\n",
    "    # hash table\n",
    "    max_hash1 = 10 * 1000000\n",
    "    H1 = np.zeros((max_hash1, ), dtype=int)\n",
    "\n",
    "    for key in readdata(k=2, report=False):\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        H1[hash_cell_1] += 1\n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "\n",
    "    for key in readdata(k=2):\n",
    "        # hash-based filtering stage from PCY\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        if H1[hash_cell_1] < s:\n",
    "            continue\n",
    "    \n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "\n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= s:\n",
    "            L2[key] = count\n",
    "    t2 = round(time.time() - t,3)\n",
    "    print('PCY:      {} items with >{} occurances. Calculated in {} sec'.format(len(L2), s, t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c18b3",
   "metadata": {},
   "source": [
    "## 4.\tResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b196e51",
   "metadata": {},
   "source": [
    "•\tPrint out relevant tables nicely, display well-annotated charts and explain if needed in plain English.\n",
    "•\tUse minimum code here, just output-functions’ calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fca140",
   "metadata": {},
   "source": [
    "1. Find the frequent pair of items (2-tuples) using the naïve, A-priori and PCY algorithms. For each of these compare the time of execution and results for supports s=10, 50, 100. Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1a67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive:    1784 items with >10 occurances. Calculated in 0.05 sec\n"
     ]
    }
   ],
   "source": [
    "for s in range(10,110,10):\n",
    "    Naive_method(s)\n",
    "    Apriori(s)\n",
    "    PCY(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc08b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbaskets = len(authors)\n",
    "interesting_pairs(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e5715",
   "metadata": {},
   "source": [
    "2. For the PCY algorithm, create up to 5 compact hash tables. What is  the difference in results and time of execution for 1,2,3,4 and 5 tables? Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=20\n",
    "s = N\n",
    "t = time.time()\n",
    "# hash table\n",
    "max_hash1 = 5*1000000-673\n",
    "max_hash2 = 5*1000000+673\n",
    "max_hash3 = 5*1000000+2*673\n",
    "max_hash4 = 5*1000000+3*673\n",
    "max_hash5 = 5*1000000+4*673\n",
    "\n",
    "\n",
    "H1 = np.zeros((max_hash1,), dtype=int)\n",
    "H2 = np.zeros((max_hash2,), dtype=int)\n",
    "H3 = np.zeros((max_hash3,), dtype=int)\n",
    "H4 = np.zeros((max_hash4,), dtype=int)\n",
    "H5 = np.zeros((max_hash5,), dtype=int)\n",
    "\n",
    "for key in readdata(k=2, report=False):\n",
    "    hash_cell_1 = hash(key) % max_hash1\n",
    "    H1[hash_cell_1] += 1\n",
    "    hash_cell_2 = hash(key) % max_hash2\n",
    "    H2[hash_cell_2] += 1\n",
    "    hash_cell_3 = hash(key) % max_hash3\n",
    "    H3[hash_cell_3] += 1\n",
    "    hash_cell_4 = hash(key) % max_hash4\n",
    "    H4[hash_cell_4] += 1\n",
    "    hash_cell_5 = hash(key) % max_hash5\n",
    "    H5[hash_cell_5] += 1\n",
    "\n",
    "# compact hash table\n",
    "H_good_1 = set(np.where(H1 >= N)[0])\n",
    "H_good_2 = set(np.where(H2 >= N)[0])\n",
    "H_good_3 = set(np.where(H3 >= N)[0])\n",
    "H_good_4 = set(np.where(H4 >= N)[0])\n",
    "H_good_5 = set(np.where(H5 >= N)[0])\n",
    "del H1\n",
    "del H2\n",
    "del H3\n",
    "del H4\n",
    "del H5\n",
    "\n",
    "# find frequent 1-tuples (individual items)\n",
    "C1 = {}\n",
    "for key in readdata(k=1, report=False):\n",
    "    if key not in C1:\n",
    "        C1[key] = 1\n",
    "    else:\n",
    "        C1[key] += 1    \n",
    "\n",
    "# filter stage\n",
    "L1 = {}\n",
    "for key, count in C1.items():\n",
    "    if count >= N:\n",
    "        L1[key] = count\n",
    "\n",
    "C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()]) # List comprehensions in python\n",
    "\n",
    "# find frequent 2-tuples\n",
    "C2 = {}\n",
    "\n",
    "\n",
    "for i in range (1,6):\n",
    "    for key in readdata(k=2):\n",
    "        # hash-based filtering stage from PCY\n",
    "        if i >= 1:\n",
    "            hash_cell_1 = hash(key) % max_hash1\n",
    "            if hash_cell_1 not in H_good_1:\n",
    "                continue\n",
    "        if i >= 2:\n",
    "            hash_cell_2 = hash(key) % max_hash2\n",
    "            if hash_cell_2 not in H_good_2:\n",
    "                continue\n",
    "        if i >= 3:\n",
    "            hash_cell_3 = hash(key) % max_hash3\n",
    "            if hash_cell_3 not in H_good_3:\n",
    "                continue\n",
    "        if i >= 4:\n",
    "            hash_cell_4 = hash(key) % max_hash4\n",
    "            if hash_cell_4 not in H_good_4:\n",
    "                continue\n",
    "        if i >= 5:\n",
    "            hash_cell_5 = hash(key) % max_hash5\n",
    "            if hash_cell_5 not in H_good_5:\n",
    "                continue\n",
    "        \n",
    "     # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "    # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "        \n",
    "#print(\"{} items\".format(len(C2)))\n",
    "\n",
    "# filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= N:\n",
    "            L2[key] = count\n",
    "    t2 = round(time.time() - t,3)\n",
    "    print('{} items with >{} occurances. Number of hash-tables: {}. Calculated in {} sec'.format(len(L2), s, i, t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c82b3",
   "metadata": {},
   "source": [
    "3. Find the final list of k-frequent items (k-tuples) for k=3 and 4. Experiment a bit and describe the best value for the support in each case. *Warning*: You can use any of the three algorithms, but be careful, because the algorithm can take too long if you don't chose it properly (well, basically don't use the naïve approach ;))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687e4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCY for 3-tuples: k = 3\n",
    "N=20\n",
    "s = N\n",
    "t = time.time()\n",
    "\n",
    "def PCY3(s):\n",
    "    t = time.time()\n",
    "    # find frequent 1-tuples (individual items)\n",
    "    C1 = {}\n",
    "    for key in readdata(k=1, report=False):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "            \n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= s:\n",
    "            L1[key] = count\n",
    "            \n",
    "# find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata(k=2, report=False):\n",
    "     # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "\n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= s:\n",
    "            L2[key] = count\n",
    "\n",
    "    C3_items = set([a.union(b) for a in L2.keys() for b in L2.keys() ]) # List comprehensions in python\n",
    "\n",
    "    # hash table\n",
    "    max_hash1 = 10 * 1000000\n",
    "    H1 = np.zeros((max_hash1, ), dtype=int)\n",
    "\n",
    "    for key in readdata(k=3, report=False):\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        H1[hash_cell_1] += 1\n",
    "    # find frequent 3-tuples\n",
    "    C3 = {}\n",
    "\n",
    "    for key in readdata(k=3):\n",
    "        # hash-based filtering stage from PCY\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        if H1[hash_cell_1] < s:\n",
    "            continue\n",
    "    \n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C3_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C3:\n",
    "            C3[key] = 1\n",
    "        else:\n",
    "            C3[key] += 1\n",
    "\n",
    "    # filter stage\n",
    "    L3 = {}\n",
    "    for key, count in C3.items():\n",
    "        if count >= s:\n",
    "            L3[key] = count\n",
    "    t3 = round(time.time() - t,3)\n",
    "    print('PCY3:      {} items with >{} occurances. Calculated in {} sec'.format(len(L3), s, t3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e094b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s=10\n",
    "PCY3(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59704f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCY for 4-tuples: k = 4\n",
    "#N=20\n",
    "#s = N\n",
    "t = time.time()\n",
    "\n",
    "def PCY4(s):\n",
    "    t = time.time()\n",
    "    # find frequent 1-tuples (individual items)\n",
    "    C1 = {}\n",
    "    for key in readdata(k=1, report=False):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "            \n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= s:\n",
    "            L1[key] = count\n",
    "            \n",
    "# find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata(k=2, report=False):\n",
    "     # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "\n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= s:\n",
    "            L2[key] = count\n",
    "\n",
    "# find frequent 3-tuples\n",
    "    C3 = {}\n",
    "    for key in readdata(k=3, report=False):\n",
    "     # record frequent tuples\n",
    "        if key not in C3:\n",
    "            C3[key] = 1\n",
    "        else:\n",
    "            C3[key] += 1\n",
    "\n",
    "    # filter stage\n",
    "    L3 = {}\n",
    "    for key, count in C3.items():\n",
    "        if count >= s:\n",
    "            L3[key] = count\n",
    "    \n",
    "    C4_items = set([a.union(b) for a in L3.keys() for b in L3.keys() ]) # List comprehensions in python\n",
    "\n",
    "    # hash table\n",
    "    max_hash1 = 10 * 1000000\n",
    "    H1 = np.zeros((max_hash1, ), dtype=int)\n",
    "\n",
    "    for key in readdata(k=4, report=False):\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        H1[hash_cell_1] += 1\n",
    "    # find frequent 3-tuples\n",
    "    C4 = {}\n",
    "\n",
    "    for key in readdata(k=4):\n",
    "        # hash-based filtering stage from PCY\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        if H1[hash_cell_1] < s:\n",
    "            continue\n",
    "    \n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C4_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C4:\n",
    "            C4[key] = 1\n",
    "        else:\n",
    "            C4[key] += 1\n",
    "\n",
    "    # filter stage\n",
    "    L4 = {}\n",
    "    for key, count in C4.items():\n",
    "        if count >= s:\n",
    "            L4[key] = count\n",
    "    t4 = round(time.time() - t,3)\n",
    "    print('PCY4:      {} items with >{} occurances. Calculated in {} sec'.format(len(L4), s, t4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92643667",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s=10\n",
    "PCY4(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d0a0bb",
   "metadata": {},
   "source": [
    "4. Using one of the results of the previous items, for one k (k=2 or 3) find the possible clusters using the 1-NN criteria. Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d969bf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating using pcy_multihash with s=10, k=2 and 2 hashtables\n",
      "Found: 263 abstracts in 0.00s\n",
      "Found: 355 authors\n",
      "Found: 42 clusters\n"
     ]
    }
   ],
   "source": [
    "s = 10\n",
    "i = 2\n",
    "k=2\n",
    "start = time.time()\n",
    "print(\"Calculating using pcy_multihash with s=%d, k=%d and %d hashtables\"%(s,k,i))\n",
    "res = C2\n",
    "print(\"Found: %d abstracts in %.2fs\"%(len(res),time.time() - start))\n",
    "\n",
    "authorset = set()\n",
    "for authors in res.keys():\n",
    "    authorset = authorset.union(authors)\n",
    "print(\"Found: %d authors\"%len(authorset))\n",
    "\n",
    "#create square bool grid to find matches\n",
    "grid = np.zeros((len(authorset),len(authorset)), bool)\n",
    "authorsmap = dict(enumerate(authorset))\n",
    "authorsinvertedmap = {v: k for k, v in enumerate(authorset)}\n",
    "for authors in res.keys():\n",
    "    for authorpair in itertools.combinations(authors, 2):\n",
    "        grid[authorsinvertedmap[authorpair[0]]][authorsinvertedmap[authorpair[1]]] = True\n",
    "\n",
    "clusters = set()\n",
    "for row in grid:\n",
    "    cluster = set()\n",
    "    for x,match in enumerate(row):\n",
    "        if(match):\n",
    "            cluster.add(authorsmap[x])\n",
    "    if len(cluster) > 1:\n",
    "        clusters.add(frozenset(cluster))\n",
    "\n",
    "print(\"Found: %d clusters\"%len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f908d306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{frozenset({'Anastasopoulos Antonios', 'Nakamura Satoshi', 'Sakti Sakriani'}),\n",
       " frozenset({'Niehues Jan', 'Waibel Alex'}),\n",
       " frozenset({\"Blain Fr{\\\\'e}d{\\\\'e}ric\", 'Scarton Carolina'}),\n",
       " frozenset({'Idiart Marco', 'Ramisch Carlos'}),\n",
       " frozenset({'Huck Matthias', 'Peitz Stephan'}),\n",
       " frozenset({'Nakamura Satoshi', 'Neubig Graham', 'Sakti Sakriani'}),\n",
       " frozenset({'Cho Eunah', 'Niehues Jan', 'Vogel Stephan'}),\n",
       " frozenset({'Li Mu', 'Zhou Ming'}),\n",
       " frozenset({'Bouillon Pierrette', 'Hockey Beth Ann'}),\n",
       " frozenset({'Li Peng', 'Zhang Jinchao'}),\n",
       " frozenset({'Liu Kang', 'Zhao Jun'}),\n",
       " frozenset({'Ney Hermann', 'Peitz Stephan'}),\n",
       " frozenset({'Wei Zhongyu', 'Zhang Qi'}),\n",
       " frozenset({'Glava{\\\\v{s}} Goran', 'Korhonen Anna', 'Reichart Roi'}),\n",
       " frozenset({'Allauzen Alexandre', 'Wisniewski Guillaume'}),\n",
       " frozenset({'Cettolo Mauro', 'Federico Marcello'}),\n",
       " frozenset({'Duan Nan', 'Zhou Ming'}),\n",
       " frozenset({'Panchenko Alexander', 'Riedl Martin', 'Yimam Seid Muhie'}),\n",
       " frozenset({'Bojar Ond{\\\\v{r}}ej',\n",
       "            'Federmann Christian',\n",
       "            'Haddow Barry',\n",
       "            'Huck Matthias',\n",
       "            'Monz Christof'}),\n",
       " frozenset({'Huang Yongfeng', 'Wu Chuhan'}),\n",
       " frozenset({'Ekbal Asif',\n",
       "            'Joshi Aditya',\n",
       "            'Kanojia Diptesh',\n",
       "            'Kunchukuttan Anoop',\n",
       "            'Saha Sriparna'}),\n",
       " frozenset({'Imamura Kenji', 'Paul Michael', 'Utiyama Masao'}),\n",
       " frozenset({'Eisner Jason', 'Pimentel Tiago'}),\n",
       " frozenset({'Smith Noah A.', 'Tsvetkov Yulia'}),\n",
       " frozenset({'Han Xu', 'Lin Yankai', 'Liu Yang', 'Liu Zhiyuan'}),\n",
       " frozenset({'Han Jiawei', 'Huang Lifu'}),\n",
       " frozenset({'Liu Ting', 'Wu Hua'}),\n",
       " frozenset({'Bohnet Bernd', 'Mille Simon'}),\n",
       " frozenset({'Bojar Ond{\\\\v{r}}ej', 'Haddow Barry'}),\n",
       " frozenset({\"Mrk{\\\\v{s}}i{\\\\'c} Nikola\", 'Wen Tsung-Hsien'}),\n",
       " frozenset({'Abdelali Ahmed', 'Darwish Kareem'}),\n",
       " frozenset({'Inoue Naoya', 'Okazaki Naoaki', 'Suzuki Jun'}),\n",
       " frozenset({'Che Wanxiang', 'Li Sheng', 'Qin Bing'}),\n",
       " frozenset({'Isahara Hitoshi', 'Murata Masaki'}),\n",
       " frozenset({'Chatterjee Rajen', 'Turchi Marco'}),\n",
       " frozenset({'Sumita Eiichiro', 'Utiyama Masao'}),\n",
       " frozenset({\"Dell{'}Orletta Felice\", 'Venturi Giulia'}),\n",
       " frozenset({'Nagata Masaaki', 'Tsukada Hajime'}),\n",
       " frozenset({'Zhang Jiajun', 'Zhou Yu'}),\n",
       " frozenset({'Asahara Masayuki', 'Shindo Hiroyuki'}),\n",
       " frozenset({'Ananiadou Sophia',\n",
       "            'Matsuzaki Takuya',\n",
       "            'Ohta Tomoko',\n",
       "            'Pyysalo Sampo'}),\n",
       " frozenset({'Lin Shouxun', 'Liu Yang', 'Way Andy'})}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5922d9",
   "metadata": {},
   "source": [
    "## 5.\tConclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d17acd",
   "metadata": {},
   "source": [
    "•\tSummarize your findings here in 5...10 lines of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb96676",
   "metadata": {},
   "source": [
    "Here is the summary of our Frequent Item Sets finding steps performed:\n",
    "\n",
    "1) Extract data from online source was our very first step. We extracted the given source data from .bib file and it needed few steps and cleaning to peform to get the required text file.\n",
    "\n",
    "2) we loaded the file and then data mining was very critical and important task: As based on how we did the data mining. the ouput of data mining is further used for calculations. And hence this step is very much important for further modelling of the data.\n",
    "\n",
    "3) We created frozensets of only authors & editors from abstract\n",
    "\n",
    "4) For frequent item sets calculation, first of all we calcualated 1-tuple sets (k=1) which is used as very first input for further frequent item sets calculcations.\n",
    "\n",
    "5) Then we used 3 different Frequent Item Sets methods namely (i) Naive (ii) A-Priori (iii) PCY. We performed Frequent Item Sets for 2-tuple (k=2) and for few support values (s) 10, 20, 100. And verified the results and peformances.\n",
    "Observation: For smaller support values (10, 20 etc.), The Naive method is much faster as compared to A-Priori & PCY. However, as the higher support value are used (s = 50 to 100), the Naive method starts taking more processing time and A-Priori & PCY processing time gets reduced.\n",
    "Hence, we can conclude, For the given data & for 2-tuple (k=2): \n",
    "-smaller the support values(s=10) faster processing time for Naive and slower processing time for A-Priori & PCY.\n",
    "-larger the support values (s=50 to 100), slower the processing time for Naive and faster the processing time for A-Priori & PCY respetively.\n",
    "\n",
    "6) We also calculated 3-tuple and 4-tuple (k=3 & k=4 respectively) frequet item sets using PCY method. And we see the results are good and peformace is fast enough.\n",
    "\n",
    "7) Finally, clustering is done for 2-tuple frozen sets (k=2) using grid serach calculations. The clusters look good.\n",
    "\n",
    "Overall, we conclude that the results of each step are satisfactory and good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "960265d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in Frequent_pairs.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 26d0de7] some progress\n",
      " 1 file changed, 268 insertions(+), 465 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/AlexTouvras/FindingSimilarItems\n",
      "   57a49d5..26d0de7  main -> main\n"
     ]
    }
   ],
   "source": [
    "! git add Frequent_pairs.ipynb\n",
    "! git commit -m \"final comments\"\n",
    "! git push "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
